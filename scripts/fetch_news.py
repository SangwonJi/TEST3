#!/usr/bin/env python3
"""
ë‰´ìŠ¤ ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸
Google News RSSë¥¼ ì‚¬ìš©í•˜ì—¬ PUBG Mobile ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê³  CSVë¡œ ì €ì¥í•©ë‹ˆë‹¤.
"""

import os
import sys
import json
import csv
import feedparser
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Optional
import time
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬
PROJECT_ROOT = Path(__file__).parent.parent
DATA_DIR = PROJECT_ROOT / 'data'
SCRIPTS_DIR = PROJECT_ROOT / 'scripts'
KEYWORDS_FILE = SCRIPTS_DIR / 'keywords.json'
NEWS_CSV = DATA_DIR / 'news.csv'

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (ë¡œì»¬ ê°œë°œ í™˜ê²½)
try:
    from dotenv import load_dotenv
    load_dotenv(PROJECT_ROOT / '.env')
    logger.info("ë¡œì»¬ í™˜ê²½: .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ")
except ImportError:
    logger.info("GitHub Actions í™˜ê²½: os.getenv ì‚¬ìš©")


def load_keywords() -> Dict:
    """í‚¤ì›Œë“œ ì„¤ì • íŒŒì¼ ë¡œë“œ"""
    try:
        with open(KEYWORDS_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        logger.error(f"í‚¤ì›Œë“œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {KEYWORDS_FILE}")
        return {
            "base_keywords": ["PUBG Mobile"],
            "country_keywords": {},
            "categories": ["gaming"]
        }
    except json.JSONDecodeError:
        logger.error("í‚¤ì›Œë“œ íŒŒì¼ JSON íŒŒì‹± ì˜¤ë¥˜")
        return {"base_keywords": ["PUBG Mobile"], "country_keywords": {}, "categories": ["gaming"]}


def get_continent(country: str) -> str:
    """êµ­ê°€ëª…ìœ¼ë¡œ ëŒ€ë¥™ ë°˜í™˜"""
    continent_map = {
        'USA': 'NORTH AMERICA', 'Canada': 'NORTH AMERICA', 'Mexico': 'NORTH AMERICA',
        'Brazil': 'SOUTH AMERICA', 'Argentina': 'SOUTH AMERICA',
        'Germany': 'EUROPE', 'UK': 'EUROPE', 'France': 'EUROPE', 'Italy': 'EUROPE', 'Spain': 'EUROPE',
        'China': 'ASIA', 'India': 'ASIA', 'Japan': 'ASIA', 'Korea': 'ASIA', 'South Korea': 'ASIA',
        'South Africa': 'AFRICA', 'Egypt': 'AFRICA', 'Nigeria': 'AFRICA',
        'Australia': 'OCEANIA', 'New Zealand': 'OCEANIA',
        'Russia': 'RUSSIA & CIS'
    }
    return continent_map.get(country, 'OTHER')


def fetch_news_from_openai(keyword: str, countries: List[Dict] = None) -> List[Dict]:
    """
    OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ê²€ìƒ‰ ë° ë¶„ì„
    
    Args:
        keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
        countries: ê´€ë ¨ êµ­ê°€ ë¦¬ìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)
    
    Returns:
        ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        logger.warning("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return []
    
    try:
        import requests
        
        # êµ­ê°€ ì •ë³´ í¬í•¨
        country_context = ""
        if countries:
            country_names = [c.get('country', '') for c in countries if c.get('country')]
            if country_names:
                country_context = f" íŠ¹íˆ {', '.join(country_names[:5])} êµ­ê°€ì™€ ê´€ë ¨ëœ"
        
        prompt = f"""ë‹¤ìŒ í‚¤ì›Œë“œì™€ ê´€ë ¨ëœ ìµœì‹  ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ê³  ë¶„ì„í•´ì£¼ì„¸ìš”: {keyword}{country_context}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš” (ìµœëŒ€ 10ê°œ):
[
  {{
    "title": "ë‰´ìŠ¤ ì œëª©",
    "summary": "ìš”ì•½ (2-3ë¬¸ì¥)",
    "url": "ë‰´ìŠ¤ ë§í¬ (ê°€ëŠ¥í•œ ê²½ìš°)",
    "source": "ì¶œì²˜",
    "date": "YYYY-MM-DD í˜•ì‹",
    "country": "ê´€ë ¨ êµ­ê°€ (ì—†ìœ¼ë©´ null)",
    "reason": "íŠ¸ë˜í”½ ë³€í™”ì™€ì˜ ì—°ê´€ì„± ë¶„ì„"
  }}
]

ìµœê·¼ 7ì¼ ì´ë‚´ì˜ ë‰´ìŠ¤ë§Œ í¬í•¨í•˜ê³ , PUBG Mobileì´ë‚˜ ëª¨ë°”ì¼ ê²Œì„ê³¼ ê´€ë ¨ëœ ë‰´ìŠ¤ë§Œ ì•Œë ¤ì£¼ì„¸ìš”."""

        response = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers={
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            },
            json={
                "model": "gpt-4o-mini",  # ë˜ëŠ” "gpt-4", "gpt-3.5-turbo"
                "messages": [
                    {"role": "system", "content": "You are a news analyst. Return only valid JSON array."},
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.7,
                "max_tokens": 2000
            },
            timeout=30
        )
        
        if response.status_code != 200:
            logger.error(f"OpenAI API ì˜¤ë¥˜: {response.status_code} - {response.text}")
            return []
        
        data = response.json()
        content = data['choices'][0]['message']['content']
        
        # JSON ì¶”ì¶œ
        import re
        json_match = re.search(r'\[[\s\S]*\]', content)
        if json_match:
            news_data = json.loads(json_match.group())
            
            # í˜•ì‹ ë³€í™˜
            news_list = []
            for item in news_data:
                news_list.append({
                    'date': item.get('date', datetime.now().strftime('%Y-%m-%d')),
                    'country': item.get('country'),
                    'continent': get_continent(item.get('country', '')) if item.get('country') else None,
                    'title': item.get('title', ''),
                    'summary': item.get('summary', '')[:500],
                    'url': item.get('url', '#'),
                    'source': item.get('source', 'OpenAI'),
                    'category': 'gaming'
                })
            
            logger.info(f"OpenAI APIë¡œ '{keyword}'ì—ì„œ {len(news_list)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
            return news_list
        else:
            logger.warning("OpenAI ì‘ë‹µì—ì„œ JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
            
    except Exception as e:
        logger.error(f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return []


def fetch_news_from_claude(keyword: str, countries: List[Dict] = None) -> List[Dict]:
    """
    Claude APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ê²€ìƒ‰ ë° ë¶„ì„
    
    Args:
        keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
        countries: ê´€ë ¨ êµ­ê°€ ë¦¬ìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)
    
    Returns:
        ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    api_key = os.getenv('CLAUDE_API_KEY')
    if not api_key:
        logger.warning("CLAUDE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return []
    
    try:
        import requests
        
        # êµ­ê°€ ì •ë³´ í¬í•¨
        country_context = ""
        if countries:
            country_names = [c.get('country', '') for c in countries if c.get('country')]
            if country_names:
                country_context = f" íŠ¹íˆ {', '.join(country_names[:5])} êµ­ê°€ì™€ ê´€ë ¨ëœ"
        
        prompt = f"""ë‹¤ìŒ í‚¤ì›Œë“œì™€ ê´€ë ¨ëœ ìµœì‹  ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ê³  ë¶„ì„í•´ì£¼ì„¸ìš”: {keyword}{country_context}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš” (ìµœëŒ€ 10ê°œ):
[
  {{
    "title": "ë‰´ìŠ¤ ì œëª©",
    "summary": "ìš”ì•½ (2-3ë¬¸ì¥)",
    "url": "ë‰´ìŠ¤ ë§í¬ (ê°€ëŠ¥í•œ ê²½ìš°)",
    "source": "ì¶œì²˜",
    "date": "YYYY-MM-DD í˜•ì‹",
    "country": "ê´€ë ¨ êµ­ê°€ (ì—†ìœ¼ë©´ null)",
    "reason": "íŠ¸ë˜í”½ ë³€í™”ì™€ì˜ ì—°ê´€ì„± ë¶„ì„"
  }}
]

ìµœê·¼ 7ì¼ ì´ë‚´ì˜ ë‰´ìŠ¤ë§Œ í¬í•¨í•˜ê³ , PUBG Mobileì´ë‚˜ ëª¨ë°”ì¼ ê²Œì„ê³¼ ê´€ë ¨ëœ ë‰´ìŠ¤ë§Œ ì•Œë ¤ì£¼ì„¸ìš”."""

        response = requests.post(
            "https://api.anthropic.com/v1/messages",
            headers={
                "x-api-key": api_key,
                "anthropic-version": "2023-06-01",
                "Content-Type": "application/json"
            },
            json={
                "model": "claude-3-5-sonnet-20241022",  # ë˜ëŠ” "claude-3-opus-20240229"
                "max_tokens": 2000,
                "messages": [
                    {"role": "user", "content": prompt}
                ]
            },
            timeout=30
        )
        
        if response.status_code != 200:
            logger.error(f"Claude API ì˜¤ë¥˜: {response.status_code} - {response.text}")
            return []
        
        data = response.json()
        content = data['content'][0]['text']
        
        # JSON ì¶”ì¶œ
        import re
        json_match = re.search(r'\[[\s\S]*\]', content)
        if json_match:
            news_data = json.loads(json_match.group())
            
            # í˜•ì‹ ë³€í™˜
            news_list = []
            for item in news_data:
                news_list.append({
                    'date': item.get('date', datetime.now().strftime('%Y-%m-%d')),
                    'country': item.get('country'),
                    'continent': get_continent(item.get('country', '')) if item.get('country') else None,
                    'title': item.get('title', ''),
                    'summary': item.get('summary', '')[:500],
                    'url': item.get('url', '#'),
                    'source': item.get('source', 'Claude'),
                    'category': 'gaming'
                })
            
            logger.info(f"Claude APIë¡œ '{keyword}'ì—ì„œ {len(news_list)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
            return news_list
        else:
            logger.warning("Claude ì‘ë‹µì—ì„œ JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
            
    except Exception as e:
        logger.error(f"Claude API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return []


def fetch_news_from_api(keyword: str, api_type: str = 'rss', countries: List[Dict] = None) -> List[Dict]:
    """
    APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸° (í™•ì¥ ê°€ëŠ¥)
    
    Args:
        keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
        api_type: API íƒ€ì… ('rss', 'openai', 'claude', 'gemini' ë“±)
        countries: ê´€ë ¨ êµ­ê°€ ë¦¬ìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)
    
    Returns:
        ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    # RSSëŠ” ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©
    if api_type == 'rss':
        return fetch_news_from_rss(keyword)
    
    # OpenAI API ì‚¬ìš©
    elif api_type == 'openai':
        news = fetch_news_from_openai(keyword, countries)
        if news:
            return news
        else:
            logger.info("OpenAI API ì‹¤íŒ¨, RSSë¡œ í´ë°±")
            return fetch_news_from_rss(keyword)
    
    # Claude API ì‚¬ìš©
    elif api_type == 'claude':
        news = fetch_news_from_claude(keyword, countries)
        if news:
            return news
        else:
            logger.info("Claude API ì‹¤íŒ¨, RSSë¡œ í´ë°±")
            return fetch_news_from_rss(keyword)
    
    # Gemini API ì‚¬ìš© (API í‚¤ í•„ìš”)
    elif api_type == 'gemini':
        api_key = os.getenv('GEMINI_API_KEY')
        if not api_key:
            logger.warning("GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. RSSë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return fetch_news_from_rss(keyword)
        
        try:
            # Gemini API í˜¸ì¶œ ë¡œì§ (ì¶”í›„ êµ¬í˜„)
            logger.info(f"Gemini API ì‚¬ìš© (í‚¤ì›Œë“œ: {keyword})")
            return []  # TODO: Gemini API êµ¬í˜„
        except Exception as e:
            logger.error(f"Gemini API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return fetch_news_from_rss(keyword)  # í´ë°±
    
    else:
        logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” API íƒ€ì…: {api_type}. RSSë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return fetch_news_from_rss(keyword)


def fetch_news_from_rss(keyword: str, max_retries: int = 3) -> List[Dict]:
    """
    Google News RSSì—ì„œ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
    
    Args:
        keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
        max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
    
    Returns:
        ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    news_list = []
    # URL ì¸ì½”ë”©ìœ¼ë¡œ íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬
    import urllib.parse
    encoded_keyword = urllib.parse.quote(keyword)
    rss_url = f"https://news.google.com/rss/search?q={encoded_keyword}&hl=ko&gl=KR&ceid=KR:ko"
    
    for attempt in range(max_retries):
        try:
            # ë¡œê·¸ì—ëŠ” í‚¤ì›Œë“œë§Œ í‘œì‹œ (URL ì „ì²´ëŠ” í‘œì‹œí•˜ì§€ ì•ŠìŒ)
            logger.info(f"RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸° ì‹œë„ {attempt + 1}/{max_retries}: í‚¤ì›Œë“œ='{keyword}'")
            feed = feedparser.parse(rss_url)
            
            if feed.bozo and feed.bozo_exception:
                logger.warning(f"RSS íŒŒì‹± ê²½ê³ : {feed.bozo_exception}")
            
            for entry in feed.entries[:10]:  # ìµœëŒ€ 10ê°œ
                # ë‚ ì§œ íŒŒì‹±
                try:
                    pub_date = datetime(*entry.published_parsed[:6])
                except (AttributeError, TypeError):
                    pub_date = datetime.now()
                
                # ì˜¤ëŠ˜ë¶€í„° 7ì¼ ì´ë‚´ ë‰´ìŠ¤ë§Œ
                if pub_date < datetime.now() - timedelta(days=7):
                    continue
                
                news_item = {
                    'date': pub_date.strftime('%Y-%m-%d'),
                    'country': None,  # í‚¤ì›Œë“œì—ì„œ ì¶”ì¶œ
                    'continent': None,
                    'title': entry.get('title', '').strip(),
                    'summary': entry.get('summary', '').strip()[:500],  # ìµœëŒ€ 500ì
                    'url': entry.get('link', ''),
                    'source': entry.get('source', {}).get('title', 'Google News'),
                    'category': 'gaming'
                }
                
                # êµ­ê°€ëª… ì¶”ì¶œ (í‚¤ì›Œë“œì—ì„œ)
                for country in load_keywords().get('country_keywords', {}).keys():
                    if country.lower() in keyword.lower():
                        news_item['country'] = country
                        news_item['continent'] = get_continent(country)
                        break
                
                news_list.append(news_item)
            
            logger.info(f"'{keyword}'ì—ì„œ {len(news_list)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
            break  # ì„±ê³µí•˜ë©´ ì¤‘ë‹¨
            
        except Exception as e:
            logger.error(f"RSS ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time.sleep(5)  # 5ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„
            else:
                logger.error(f"'{keyword}' í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨")
    
    return news_list


def cross_validate_news(openai_news: List[Dict], claude_news: List[Dict]) -> List[Dict]:
    """
    OpenAIì™€ Claude API ê²°ê³¼ë¥¼ êµì°¨ê²€ì¦í•˜ì—¬ ì‹ ë¢°ë„ ë†’ì€ ë‰´ìŠ¤ ë°˜í™˜
    
    Args:
        openai_news: OpenAI APIë¡œ ìˆ˜ì§‘í•œ ë‰´ìŠ¤
        claude_news: Claude APIë¡œ ìˆ˜ì§‘í•œ ë‰´ìŠ¤
    
    Returns:
        êµì°¨ê²€ì¦ëœ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ (ì‹ ë¢°ë„ ì ìˆ˜ í¬í•¨)
    """
    validated_news = []
    seen_titles = set()
    
    # ì œëª© ìœ ì‚¬ë„ ë¹„êµ í•¨ìˆ˜ (ê°„ë‹¨í•œ ë²„ì „)
    def title_similarity(title1: str, title2: str) -> float:
        """ë‘ ì œëª©ì˜ ìœ ì‚¬ë„ ê³„ì‚° (0.0 ~ 1.0)"""
        title1_lower = title1.lower()
        title2_lower = title2.lower()
        
        # ì™„ì „ ì¼ì¹˜
        if title1_lower == title2_lower:
            return 1.0
        
        # ë‹¨ì–´ ê¸°ë°˜ ìœ ì‚¬ë„
        words1 = set(title1_lower.split())
        words2 = set(title2_lower.split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union) if union else 0.0
    
    # OpenAI ë‰´ìŠ¤ ì²˜ë¦¬
    for news in openai_news:
        title = news.get('title', '').lower()
        if title in seen_titles:
            continue
        
        # Claude ê²°ê³¼ì™€ ë¹„êµ
        matched = False
        best_match_score = 0.0
        best_match = None
        
        for claude_item in claude_news:
            claude_title = claude_item.get('title', '').lower()
            similarity = title_similarity(title, claude_title)
            
            if similarity > 0.7:  # 70% ì´ìƒ ìœ ì‚¬í•˜ë©´ ì¼ì¹˜ë¡œ ê°„ì£¼
                matched = True
                if similarity > best_match_score:
                    best_match_score = similarity
                    best_match = claude_item
        
        if matched and best_match:
            # ë‘ APIê°€ ì¼ì¹˜í•˜ëŠ” ë‰´ìŠ¤: ì‹ ë¢°ë„ ë†’ìŒ
            validated_item = news.copy()
            validated_item['confidence'] = 'high'
            validated_item['validation'] = f"OpenAI + Claude ì¼ì¹˜ (ìœ ì‚¬ë„: {best_match_score:.0%})"
            validated_item['openai_summary'] = news.get('summary', '')
            validated_item['claude_summary'] = best_match.get('summary', '')
            # ë” ê¸´ ìš”ì•½ ì‚¬ìš©
            if len(best_match.get('summary', '')) > len(news.get('summary', '')):
                validated_item['summary'] = best_match.get('summary', '')
            validated_news.append(validated_item)
            seen_titles.add(title)
            seen_titles.add(best_match.get('title', '').lower())
        else:
            # OpenAIë§Œ ì°¾ì€ ë‰´ìŠ¤: ì‹ ë¢°ë„ ì¤‘ê°„
            validated_item = news.copy()
            validated_item['confidence'] = 'medium'
            validated_item['validation'] = 'OpenAI only'
            validated_news.append(validated_item)
            seen_titles.add(title)
    
    # Claudeë§Œ ì°¾ì€ ë‰´ìŠ¤ ì¶”ê°€
    for news in claude_news:
        title = news.get('title', '').lower()
        if title not in seen_titles:
            validated_item = news.copy()
            validated_item['confidence'] = 'medium'
            validated_item['validation'] = 'Claude only'
            validated_news.append(validated_item)
            seen_titles.add(title)
    
    # ì‹ ë¢°ë„ ìˆœìœ¼ë¡œ ì •ë ¬ (high > medium)
    validated_news.sort(key=lambda x: (x.get('confidence') == 'high', x.get('title', '')))
    
    logger.info(f"êµì°¨ê²€ì¦ ì™„ë£Œ: ì´ {len(validated_news)}ê°œ (High: {sum(1 for n in validated_news if n.get('confidence') == 'high')}, Medium: {sum(1 for n in validated_news if n.get('confidence') == 'medium')})")
    
    return validated_news


def fetch_news_with_cross_validation(keyword: str, countries: List[Dict] = None) -> List[Dict]:
    """
    OpenAIì™€ Claude APIë¥¼ ëª¨ë‘ ì‚¬ìš©í•˜ì—¬ êµì°¨ê²€ì¦
    
    Args:
        keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
        countries: ê´€ë ¨ êµ­ê°€ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        êµì°¨ê²€ì¦ëœ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    logger.info(f"êµì°¨ê²€ì¦ ì‹œì‘: {keyword}")
    
    # ë‘ API ëª¨ë‘ í˜¸ì¶œ
    openai_news = fetch_news_from_openai(keyword, countries)
    time.sleep(1)  # API ë¶€í•˜ ë°©ì§€
    claude_news = fetch_news_from_claude(keyword, countries)
    
    # êµì°¨ê²€ì¦
    if openai_news or claude_news:
        validated = cross_validate_news(openai_news, claude_news)
        return validated
    else:
        # ë‘˜ ë‹¤ ì‹¤íŒ¨ ì‹œ RSS í´ë°±
        logger.warning("OpenAIì™€ Claude ëª¨ë‘ ì‹¤íŒ¨, RSSë¡œ í´ë°±")
        return fetch_news_from_rss(keyword)


def remove_duplicates(existing_news: List[Dict], new_news: List[Dict]) -> List[Dict]:
    """
    ì¤‘ë³µ ë‰´ìŠ¤ ì œê±° (URL ê¸°ì¤€)
    
    Args:
        existing_news: ê¸°ì¡´ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        new_news: ìƒˆë¡œ ìˆ˜ì§‘í•œ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        ì¤‘ë³µ ì œê±°ëœ ìƒˆ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    existing_urls = {news['url'] for news in existing_news if news.get('url')}
    unique_news = [news for news in new_news if news.get('url') not in existing_urls]
    
    logger.info(f"ì¤‘ë³µ ì œê±°: {len(new_news)}ê°œ ì¤‘ {len(unique_news)}ê°œ ìœ ë‹ˆí¬")
    return unique_news


def load_existing_news() -> List[Dict]:
    """ê¸°ì¡´ CSV íŒŒì¼ì—ì„œ ë‰´ìŠ¤ ë¡œë“œ"""
    if not NEWS_CSV.exists():
        logger.info("ê¸°ì¡´ ë‰´ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
        return []
    
    try:
        df = pd.read_csv(NEWS_CSV, encoding='utf-8-sig')
        return df.to_dict('records')
    except Exception as e:
        logger.error(f"ê¸°ì¡´ ë‰´ìŠ¤ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
        return []


def save_to_csv(all_news: List[Dict]):
    """
    ë‰´ìŠ¤ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥
    
    Args:
        all_news: ì €ì¥í•  ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    if not all_news:
        logger.warning("ì €ì¥í•  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    try:
        # ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±
        DATA_DIR.mkdir(exist_ok=True)
        
        # DataFrame ìƒì„±
        df = pd.DataFrame(all_news)
        
        # ì»¬ëŸ¼ ìˆœì„œ ì§€ì • (êµì°¨ê²€ì¦ ì»¬ëŸ¼ í¬í•¨)
        base_columns = ['date', 'country', 'continent', 'title', 'summary', 'url', 'source', 'category']
        optional_columns = ['confidence', 'validation', 'openai_summary', 'claude_summary']
        
        # ëª¨ë“  ì»¬ëŸ¼ í™•ì¸
        all_columns = base_columns + [col for col in optional_columns if col in df.columns]
        df = df.reindex(columns=all_columns)
        
        # ë‚ ì§œìˆœ ì •ë ¬ (ìµœì‹ ìˆœ)
        df['date'] = pd.to_datetime(df['date'])
        df = df.sort_values('date', ascending=False)
        df['date'] = df['date'].dt.strftime('%Y-%m-%d')
        
        # CSV ì €ì¥ (UTF-8 with BOM for Excel compatibility)
        df.to_csv(NEWS_CSV, index=False, encoding='utf-8-sig', quoting=csv.QUOTE_MINIMAL)
        
        logger.info(f"ë‰´ìŠ¤ {len(all_news)}ê°œë¥¼ {NEWS_CSV}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        logger.error(f"CSV ì €ì¥ ì‹¤íŒ¨: {e}")
        raise


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger.info("=" * 50)
    logger.info("ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")
    logger.info("=" * 50)
    
    try:
        # í‚¤ì›Œë“œ ë¡œë“œ
        keywords_config = load_keywords()
        base_keywords = keywords_config.get('base_keywords', [])
        country_keywords = keywords_config.get('country_keywords', {})
        
        # ê¸°ì¡´ ë‰´ìŠ¤ ë¡œë“œ
        existing_news = load_existing_news()
        logger.info(f"ê¸°ì¡´ ë‰´ìŠ¤: {len(existing_news)}ê°œ")
        
        # ìƒˆ ë‰´ìŠ¤ ìˆ˜ì§‘
        all_new_news = []
        
        # API íƒ€ì… ê²°ì • (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì½ê¸°, ê¸°ë³¸ê°’: 'rss')
        api_type = os.getenv('NEWS_API_TYPE', 'rss').lower()
        
        # êµì°¨ê²€ì¦ ëª¨ë“œ í™•ì¸
        cross_validate = os.getenv('CROSS_VALIDATE', 'false').lower() == 'true'
        
        if cross_validate and api_type in ['openai', 'claude']:
            logger.info("ğŸ” êµì°¨ê²€ì¦ ëª¨ë“œ: OpenAIì™€ Claude API ëª¨ë‘ ì‚¬ìš©")
        else:
            logger.info(f"ì‚¬ìš©í•  API íƒ€ì…: {api_type}")
        
        # ìƒìŠ¹/í•˜ë½ êµ­ê°€ ì •ë³´ ì¤€ë¹„ (AI API ì‚¬ìš© ì‹œ ì»¨í…ìŠ¤íŠ¸ë¡œ ì „ë‹¬)
        risers_fallers = None
        if api_type in ['openai', 'claude'] or cross_validate:
            # íŠ¸ë˜í”½ ë°ì´í„°ê°€ ìˆë‹¤ë©´ ìƒìŠ¹/í•˜ë½ êµ­ê°€ ì •ë³´ ì „ë‹¬
            # (í˜„ì¬ëŠ” í‚¤ì›Œë“œ ê¸°ë°˜ì´ì§€ë§Œ, ì¶”í›„ í™•ì¥ ê°€ëŠ¥)
            risers_fallers = []
        
        # ê¸°ë³¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
        for keyword in base_keywords:
            if cross_validate and api_type in ['openai', 'claude']:
                # êµì°¨ê²€ì¦: ë‘ API ëª¨ë‘ ì‚¬ìš©
                news = fetch_news_with_cross_validation(keyword, risers_fallers)
            else:
                news = fetch_news_from_api(keyword, api_type, risers_fallers)
            all_new_news.extend(news)
            time.sleep(2)  # API ë¶€í•˜ ë°©ì§€
        
        # êµ­ê°€ë³„ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
        for country, country_keyword_list in country_keywords.items():
            for keyword in country_keyword_list:
                # êµ­ê°€ë³„ ê²€ìƒ‰ ì‹œ í•´ë‹¹ êµ­ê°€ ì •ë³´ ì „ë‹¬
                country_context = [{'country': country}] if (api_type in ['openai', 'claude'] or cross_validate) else None
                
                if cross_validate and api_type in ['openai', 'claude']:
                    news = fetch_news_with_cross_validation(keyword, country_context)
                else:
                    news = fetch_news_from_api(keyword, api_type, country_context)
                all_new_news.extend(news)
                time.sleep(2)
        
        # ì¤‘ë³µ ì œê±°
        unique_new_news = remove_duplicates(existing_news, all_new_news)
        
        # ê¸°ì¡´ ë‰´ìŠ¤ì™€ í•©ì¹˜ê¸°
        all_news = existing_news + unique_new_news
        
        # ì €ì¥
        if unique_new_news:
            save_to_csv(all_news)
            logger.info(f"âœ… {len(unique_new_news)}ê°œì˜ ìƒˆ ë‰´ìŠ¤ë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
            return 0  # ì„±ê³µ (ë³€ê²½ì‚¬í•­ ìˆìŒ)
        else:
            logger.info("ìƒˆë¡œìš´ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return 1  # ë³€ê²½ì‚¬í•­ ì—†ìŒ
            
    except Exception as e:
        logger.error(f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        return -1  # ì‹¤íŒ¨


if __name__ == '__main__':
    exit_code = main()
    sys.exit(exit_code)

