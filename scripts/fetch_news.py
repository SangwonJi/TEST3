#!/usr/bin/env python3
"""
ë‰´ìŠ¤ ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸
Google News RSSë¥¼ ì‚¬ìš©í•˜ì—¬ PUBG Mobile ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê³  CSVë¡œ ì €ì¥í•©ë‹ˆë‹¤.
"""

import os
import sys
import json
import csv
import feedparser
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Optional
import time
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬
PROJECT_ROOT = Path(__file__).parent.parent
DATA_DIR = PROJECT_ROOT / 'data'
SCRIPTS_DIR = PROJECT_ROOT / 'scripts'
KEYWORDS_FILE = SCRIPTS_DIR / 'keywords.json'
NEWS_CSV = DATA_DIR / 'news.csv'

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (ë¡œì»¬ ê°œë°œ í™˜ê²½)
try:
    from dotenv import load_dotenv
    load_dotenv(PROJECT_ROOT / '.env')
    logger.info("ë¡œì»¬ í™˜ê²½: .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ")
except ImportError:
    logger.info("GitHub Actions í™˜ê²½: os.getenv ì‚¬ìš©")


def load_keywords() -> Dict:
    """í‚¤ì›Œë“œ ì„¤ì • íŒŒì¼ ë¡œë“œ"""
    try:
        with open(KEYWORDS_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        logger.error(f"í‚¤ì›Œë“œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {KEYWORDS_FILE}")
        return {
            "base_keywords": ["PUBG Mobile"],
            "country_keywords": {},
            "categories": ["gaming"]
        }
    except json.JSONDecodeError:
        logger.error("í‚¤ì›Œë“œ íŒŒì¼ JSON íŒŒì‹± ì˜¤ë¥˜")
        return {"base_keywords": ["PUBG Mobile"], "country_keywords": {}, "categories": ["gaming"]}


def get_continent(country: str) -> str:
    """êµ­ê°€ëª…ìœ¼ë¡œ ëŒ€ë¥™ ë°˜í™˜"""
    continent_map = {
        'USA': 'NORTH AMERICA', 'Canada': 'NORTH AMERICA', 'Mexico': 'NORTH AMERICA',
        'Brazil': 'SOUTH AMERICA', 'Argentina': 'SOUTH AMERICA',
        'Germany': 'EUROPE', 'UK': 'EUROPE', 'France': 'EUROPE', 'Italy': 'EUROPE', 'Spain': 'EUROPE',
        'China': 'ASIA', 'India': 'ASIA', 'Japan': 'ASIA', 'Korea': 'ASIA', 'South Korea': 'ASIA',
        'South Africa': 'AFRICA', 'Egypt': 'AFRICA', 'Nigeria': 'AFRICA',
        'Australia': 'OCEANIA', 'New Zealand': 'OCEANIA',
        'Russia': 'RUSSIA & CIS'
    }
    return continent_map.get(country, 'OTHER')


def map_to_group_category(detail_category: str) -> str:
    """
    ì„¸ë¶€ ì¹´í…Œê³ ë¦¬ë¥¼ ê·¸ë£¹ ì¹´í…Œê³ ë¦¬ë¡œ ë§¤í•‘
    
    Args:
        detail_category: ì„¸ë¶€ ì¹´í…Œê³ ë¦¬ (ì˜ˆ: internet_shutdown, war_conflict ë“±)
    
    Returns:
        ê·¸ë£¹ ì¹´í…Œê³ ë¦¬ (outage_block, social_crisis, seasonal_calendar, gaming_competitor, other)
    """
    # ğŸ”´ ì¥ì•  ë° ì°¨ë‹¨ (Outage & Block)
    outage_block = [
        'internet_shutdown', 'tech_outage', 'power_outage', 'censorship',
        'cyber_attack', 'infrastructure_damage'
    ]
    
    # ğŸŸ  ì‚¬íšŒì  ìœ„ê¸° (Social Crisis)
    social_crisis = [
        'war_conflict', 'terrorism_explosion', 'natural_disaster',
        'protest_strike', 'curfew', 'pandemic', 'economic'
    ]
    
    # ğŸŸ¢ ì‹œì¦Œ ë° ì¼ì • (Seasonal & Calendar)
    seasonal_calendar = [
        'holiday', 'school_calendar', 'election'
    ]
    
    # ğŸ”µ ê²Œì„ ë° ê²½ìŸ (Gaming & Competitor)
    gaming_competitor = [
        'gaming', 'competitor_game', 'social_trend', 'sports_event', 'major_event'
    ]
    
    if detail_category in outage_block:
        return 'outage_block'
    elif detail_category in social_crisis:
        return 'social_crisis'
    elif detail_category in seasonal_calendar:
        return 'seasonal_calendar'
    elif detail_category in gaming_competitor:
        return 'gaming_competitor'
    else:
        return 'other'


def fetch_news_from_openai(keyword: str, countries: List[Dict] = None) -> List[Dict]:
    """
    OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ê²€ìƒ‰ ë° ë¶„ì„
    
    Args:
        keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
        countries: ê´€ë ¨ êµ­ê°€ ë¦¬ìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)
    
    Returns:
        ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        logger.warning("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return []
    
    try:
        import requests
        
        # êµ­ê°€ ì •ë³´ í¬í•¨
        country_context = ""
        if countries:
            country_names = [c.get('country', '') for c in countries if c.get('country')]
            if country_names:
                country_context = f" íŠ¹íˆ {', '.join(country_names[:5])} êµ­ê°€ì™€ ê´€ë ¨ëœ"
        
        prompt = f"""ë‹¤ìŒ í‚¤ì›Œë“œì™€ ê´€ë ¨ëœ ìµœì‹  ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ê³  ë¶„ì„í•´ì£¼ì„¸ìš”: {keyword}{country_context}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš” (ìµœëŒ€ 10ê°œ):
[
  {{
    "title": "ë‰´ìŠ¤ ì œëª©",
    "summary": "ìš”ì•½ (2-3ë¬¸ì¥)",
    "url": "ë‰´ìŠ¤ ë§í¬ (ê°€ëŠ¥í•œ ê²½ìš°)",
    "source": "ì¶œì²˜",
    "date": "YYYY-MM-DD í˜•ì‹",
    "country": "ê´€ë ¨ êµ­ê°€ (ì—†ìœ¼ë©´ null)",
    "reason": "íŠ¸ë˜í”½ ë³€í™”ì™€ì˜ ì—°ê´€ì„± ë¶„ì„"
  }}
]

ìµœê·¼ 7ì¼ ì´ë‚´ì˜ ë‰´ìŠ¤ë§Œ í¬í•¨í•˜ê³ , PUBG Mobileì´ë‚˜ ëª¨ë°”ì¼ ê²Œì„ê³¼ ê´€ë ¨ëœ ë‰´ìŠ¤ë§Œ ì•Œë ¤ì£¼ì„¸ìš”."""

        response = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers={
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            },
            json={
                "model": "gpt-4o-mini",  # ë˜ëŠ” "gpt-4", "gpt-3.5-turbo"
                "messages": [
                    {"role": "system", "content": "You are a news analyst. Return only valid JSON array."},
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.7,
                "max_tokens": 2000
            },
            timeout=30
        )
        
        if response.status_code != 200:
            logger.error(f"OpenAI API ì˜¤ë¥˜: {response.status_code} - {response.text}")
            return []
        
        data = response.json()
        content = data['choices'][0]['message']['content']
        
        # JSON ì¶”ì¶œ
        import re
        json_match = re.search(r'\[[\s\S]*\]', content)
        if json_match:
            news_data = json.loads(json_match.group())
            
            # í˜•ì‹ ë³€í™˜
            news_list = []
            for item in news_data:
                news_list.append({
                    'date': item.get('date', datetime.now().strftime('%Y-%m-%d')),
                    'country': item.get('country'),
                    'continent': get_continent(item.get('country', '')) if item.get('country') else None,
                    'title': item.get('title', ''),
                    'summary': item.get('summary', '')[:500],
                    'url': item.get('url', '#'),
                    'source': item.get('source', 'OpenAI'),
                    'category': 'gaming'
                })
            
            logger.info(f"OpenAI APIë¡œ '{keyword}'ì—ì„œ {len(news_list)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
            return news_list
        else:
            logger.warning("OpenAI ì‘ë‹µì—ì„œ JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
            
    except Exception as e:
        logger.error(f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return []


def fetch_news_from_claude(keyword: str, countries: List[Dict] = None) -> List[Dict]:
    """
    Claude APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ê²€ìƒ‰ ë° ë¶„ì„
    
    Args:
        keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
        countries: ê´€ë ¨ êµ­ê°€ ë¦¬ìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)
    
    Returns:
        ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    # CLAUDE_API_KEY ë˜ëŠ” ANTHROPIC_API_KEY ì§€ì› (ë‘˜ ë‹¤ ë™ì¼)
    api_key = os.getenv('CLAUDE_API_KEY') or os.getenv('ANTHROPIC_API_KEY')
    if not api_key:
        logger.warning("CLAUDE_API_KEY ë˜ëŠ” ANTHROPIC_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return []
    
    try:
        import requests
        
        # êµ­ê°€ ì •ë³´ í¬í•¨
        country_context = ""
        if countries:
            country_names = [c.get('country', '') for c in countries if c.get('country')]
            if country_names:
                country_context = f" íŠ¹íˆ {', '.join(country_names[:5])} êµ­ê°€ì™€ ê´€ë ¨ëœ"
        
        prompt = f"""ë‹¤ìŒ í‚¤ì›Œë“œì™€ ê´€ë ¨ëœ ìµœì‹  ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ê³  ë¶„ì„í•´ì£¼ì„¸ìš”: {keyword}{country_context}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš” (ìµœëŒ€ 10ê°œ):
[
  {{
    "title": "ë‰´ìŠ¤ ì œëª©",
    "summary": "ìš”ì•½ (2-3ë¬¸ì¥)",
    "url": "ë‰´ìŠ¤ ë§í¬ (ê°€ëŠ¥í•œ ê²½ìš°)",
    "source": "ì¶œì²˜",
    "date": "YYYY-MM-DD í˜•ì‹",
    "country": "ê´€ë ¨ êµ­ê°€ (ì—†ìœ¼ë©´ null)",
    "reason": "íŠ¸ë˜í”½ ë³€í™”ì™€ì˜ ì—°ê´€ì„± ë¶„ì„"
  }}
]

ìµœê·¼ 7ì¼ ì´ë‚´ì˜ ë‰´ìŠ¤ë§Œ í¬í•¨í•˜ê³ , PUBG Mobileì´ë‚˜ ëª¨ë°”ì¼ ê²Œì„ê³¼ ê´€ë ¨ëœ ë‰´ìŠ¤ë§Œ ì•Œë ¤ì£¼ì„¸ìš”."""

        response = requests.post(
            "https://api.anthropic.com/v1/messages",
            headers={
                "x-api-key": api_key,
                "anthropic-version": "2023-06-01",
                "Content-Type": "application/json"
            },
            json={
                "model": "claude-3-5-sonnet-20241022",  # ë˜ëŠ” "claude-3-opus-20240229"
                "max_tokens": 2000,
                "messages": [
                    {"role": "user", "content": prompt}
                ]
            },
            timeout=30
        )
        
        if response.status_code != 200:
            logger.error(f"Claude API ì˜¤ë¥˜: {response.status_code} - {response.text}")
            return []
        
        data = response.json()
        content = data['content'][0]['text']
        
        # JSON ì¶”ì¶œ
        import re
        json_match = re.search(r'\[[\s\S]*\]', content)
        if json_match:
            news_data = json.loads(json_match.group())
            
            # í˜•ì‹ ë³€í™˜
            news_list = []
            for item in news_data:
                news_list.append({
                    'date': item.get('date', datetime.now().strftime('%Y-%m-%d')),
                    'country': item.get('country'),
                    'continent': get_continent(item.get('country', '')) if item.get('country') else None,
                    'title': item.get('title', ''),
                    'summary': item.get('summary', '')[:500],
                    'url': item.get('url', '#'),
                    'source': item.get('source', 'Claude'),
                    'category': 'gaming'
                })
            
            logger.info(f"Claude APIë¡œ '{keyword}'ì—ì„œ {len(news_list)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
            return news_list
        else:
            logger.warning("Claude ì‘ë‹µì—ì„œ JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
            
    except Exception as e:
        logger.error(f"Claude API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return []


def fetch_news_from_api(keyword: str, api_type: str = 'rss', countries: List[Dict] = None) -> List[Dict]:
    """
    APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸° (í™•ì¥ ê°€ëŠ¥)
    
    Args:
        keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
        api_type: API íƒ€ì… ('rss', 'openai', 'claude', 'gemini' ë“±)
        countries: ê´€ë ¨ êµ­ê°€ ë¦¬ìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)
    
    Returns:
        ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    # RSSëŠ” ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©
    if api_type == 'rss':
        return fetch_news_from_rss(keyword)
    
    # OpenAI API ì‚¬ìš©
    elif api_type == 'openai':
        news = fetch_news_from_openai(keyword, countries)
        if news:
            return news
        else:
            logger.info("OpenAI API ì‹¤íŒ¨, RSSë¡œ í´ë°±")
            return fetch_news_from_rss(keyword)
    
    # Claude API ì‚¬ìš©
    elif api_type == 'claude':
        news = fetch_news_from_claude(keyword, countries)
        if news:
            return news
        else:
            logger.info("Claude API ì‹¤íŒ¨, RSSë¡œ í´ë°±")
            return fetch_news_from_rss(keyword)
    
    # Gemini API ì‚¬ìš© (API í‚¤ í•„ìš”)
    elif api_type == 'gemini':
        api_key = os.getenv('GEMINI_API_KEY')
        if not api_key:
            logger.warning("GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. RSSë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return fetch_news_from_rss(keyword)
        
        try:
            # Gemini API í˜¸ì¶œ ë¡œì§ (ì¶”í›„ êµ¬í˜„)
            logger.info(f"Gemini API ì‚¬ìš© (í‚¤ì›Œë“œ: {keyword})")
            return []  # TODO: Gemini API êµ¬í˜„
        except Exception as e:
            logger.error(f"Gemini API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return fetch_news_from_rss(keyword)  # í´ë°±
    
    else:
        logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” API íƒ€ì…: {api_type}. RSSë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return fetch_news_from_rss(keyword)


def fetch_news_from_rss(keyword: str, max_retries: int = 3) -> List[Dict]:
    """
    Google News RSSì—ì„œ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
    
    Args:
        keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
        max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
    
    Returns:
        ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    news_list = []
    # URL ì¸ì½”ë”©ìœ¼ë¡œ íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬
    import urllib.parse
    encoded_keyword = urllib.parse.quote(keyword)
    rss_url = f"https://news.google.com/rss/search?q={encoded_keyword}&hl=ko&gl=KR&ceid=KR:ko"
    
    for attempt in range(max_retries):
        try:
            # ë¡œê·¸ì—ëŠ” í‚¤ì›Œë“œë§Œ í‘œì‹œ (URL ì „ì²´ëŠ” í‘œì‹œí•˜ì§€ ì•ŠìŒ)
            logger.info(f"RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸° ì‹œë„ {attempt + 1}/{max_retries}: í‚¤ì›Œë“œ='{keyword}'")
            feed = feedparser.parse(rss_url)
            
            if feed.bozo and feed.bozo_exception:
                logger.warning(f"RSS íŒŒì‹± ê²½ê³ : {feed.bozo_exception}")
            
            for entry in feed.entries[:10]:  # ìµœëŒ€ 10ê°œ
                # ë‚ ì§œ íŒŒì‹±
                try:
                    pub_date = datetime(*entry.published_parsed[:6])
                except (AttributeError, TypeError):
                    pub_date = datetime.now()
                
                # ì˜¤ëŠ˜ë¶€í„° 7ì¼ ì´ë‚´ ë‰´ìŠ¤ë§Œ
                if pub_date < datetime.now() - timedelta(days=7):
                    continue
                
                news_item = {
                    'date': pub_date.strftime('%Y-%m-%d'),
                    'country': None,  # í‚¤ì›Œë“œì—ì„œ ì¶”ì¶œ
                    'continent': None,
                    'title': entry.get('title', '').strip(),
                    'summary': entry.get('summary', '').strip()[:500],  # ìµœëŒ€ 500ì
                    'url': entry.get('link', ''),
                    'source': entry.get('source', {}).get('title', 'Google News'),
                    'category': 'gaming'
                }
                
                # êµ­ê°€ëª… ì¶”ì¶œ (í‚¤ì›Œë“œì—ì„œ)
                for country in load_keywords().get('country_keywords', {}).keys():
                    if country.lower() in keyword.lower():
                        news_item['country'] = country
                        news_item['continent'] = get_continent(country)
                        break
                
                news_list.append(news_item)
            
            logger.info(f"'{keyword}'ì—ì„œ {len(news_list)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
            break  # ì„±ê³µí•˜ë©´ ì¤‘ë‹¨
            
        except Exception as e:
            logger.error(f"RSS ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time.sleep(5)  # 5ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„
            else:
                logger.error(f"'{keyword}' í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨")
    
    return news_list


def refine_news_with_ai(news_item: Dict, api_type: str = 'openai') -> Optional[Dict]:
    """
    RSSë¡œ ìˆ˜ì§‘í•œ ë‰´ìŠ¤ë¥¼ AIë¡œ ì •ì œ (ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜, íŠ¸ë˜í”½ ì˜í–¥ ë¶„ì„)
    
    Args:
        news_item: RSSë¡œ ìˆ˜ì§‘í•œ ë‰´ìŠ¤ ì•„ì´í…œ
        api_type: ì‚¬ìš©í•  API ('openai' ë˜ëŠ” 'claude')
    
    Returns:
        ì •ì œëœ ë‰´ìŠ¤ ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” None (ê´€ë ¨ ì—†ìŒ)
    """
    api_key = None
    api_url = None
    headers = {}
    payload = {}
    
    if api_type == 'openai':
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key:
            return news_item  # API í‚¤ ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜
        
        api_url = "https://api.openai.com/v1/chat/completions"
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        prompt = f"""ë‹¤ìŒ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ì—¬ ëª¨ë°”ì¼ ê²Œì„ íŠ¸ë˜í”½ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆëŠ”ì§€ íŒë‹¨í•´ì£¼ì„¸ìš”:

ì œëª©: {news_item.get('title', '')}
ë‚´ìš©: {news_item.get('summary', '')}
URL: {news_item.get('url', '')}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
  "relevant": true ë˜ëŠ” false (ëª¨ë°”ì¼ ê²Œì„ íŠ¸ë˜í”½ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìœ¼ë©´ true),
  "category": "ì„¸ë¶€ ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ (ì•„ë˜ ëª©ë¡ ì°¸ê³ )",
  "country": "ê´€ë ¨ êµ­ê°€ëª… (ì—†ìœ¼ë©´ null)",
  "traffic_impact": "íŠ¸ë˜í”½ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ì„¤ëª… (ê°„ë‹¨íˆ)",
  "summary_kr": "í•œêµ­ì–´ë¡œ 2-3ì¤„ ìš”ì•½"
}}

ì„¸ë¶€ ì¹´í…Œê³ ë¦¬ ëª©ë¡ (ì •í™•íˆ í•˜ë‚˜ ì„ íƒ):

ğŸ”´ ì¥ì•  ë° ì°¨ë‹¨ (Outage & Block):
- internet_shutdown: ì¸í„°ë„· ì°¨ë‹¨, í†µì‹  ì¥ì• 
- tech_outage: ì†Œì…œë¯¸ë””ì–´/ì•±ìŠ¤í† ì–´/í´ë¼ìš°ë“œ ì¥ì• 
- power_outage: ì •ì „, ì „ë ¥ ê³µê¸‰ ì¤‘ë‹¨
- censorship: ê²€ì—´, ì•±/ê²Œì„ ê¸ˆì§€
- cyber_attack: ì‚¬ì´ë²„ ê³µê²©, DDoS, í•´í‚¹
- infrastructure_damage: ì¸í”„ë¼ ì†ìƒ, êµëŸ‰/ê±´ë¬¼ ë¶•ê´´

ğŸŸ  ì‚¬íšŒì  ìœ„ê¸° (Social Crisis):
- war_conflict: ì „ìŸ, ë¶„ìŸ, êµ°ì‚¬ ì‘ì „
- terrorism_explosion: í…ŒëŸ¬, í­ë°œ, í­íƒ„ ê³µê²©
- natural_disaster: ì§€ì§„, í™ìˆ˜, íƒœí’, ì‚°ë¶ˆ ë“± ì²œì¬ì§€ë³€
- protest_strike: ì‹œìœ„, íŒŒì—…, í­ë™
- curfew: í†µê¸ˆ, ë´‰ì‡„, ë¹„ìƒì‚¬íƒœ
- pandemic: íŒ¬ë°ë¯¹, ì „ì—¼ë³‘, ê²©ë¦¬
- economic: ê²½ì œ ìœ„ê¸°, ì¸í”Œë ˆì´ì…˜, í†µí™” í‰ê°€ì ˆí•˜

ğŸŸ¢ ì‹œì¦Œ ë° ì¼ì • (Seasonal & Calendar):
- holiday: ê³µíœ´ì¼, ëª…ì ˆ, ì¶•ì œ
- school_calendar: ë°©í•™, ì‹œí—˜ê¸°ê°„ ë“± í•™ì‚¬ì¼ì •
- election: ì„ ê±°, íˆ¬í‘œ, ì •ì¹˜ ì´ë²¤íŠ¸

ğŸ”µ ê²Œì„ ë° ê²½ìŸ (Gaming & Competitor):
- gaming: ê²Œì„ ê´€ë ¨ ë‰´ìŠ¤
- competitor_game: ê²½ìŸ ê²Œì„ ì¶œì‹œ/ì—…ë°ì´íŠ¸
- social_trend: ë°”ì´ëŸ´ íŠ¸ë Œë“œ, ì¸í”Œë£¨ì–¸ì„œ, eìŠ¤í¬ì¸  í† ë„ˆë¨¼íŠ¸
- sports_event: ì›”ë“œì»µ, ì˜¬ë¦¼í”½ ë“± ìŠ¤í¬ì¸  ì´ë²¤íŠ¸
- major_event: ì£¼ìš” ë¬¸í™” í–‰ì‚¬, ê²Œì„ ì»¨ë²¤ì…˜

âšª ê¸°íƒ€:
- other: ë¶„ë¥˜ ë¶ˆê°€

ê´€ë ¨ì´ ì—†ìœ¼ë©´ relevant: falseë¡œ ì„¤ì •í•˜ì„¸ìš”."""

        payload = {
            "model": "gpt-4o-mini",
            "messages": [
                {"role": "system", "content": "You are a news analyst. Return only valid JSON."},
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.7,
            "max_tokens": 500
        }
    
    elif api_type == 'claude':
        api_key = os.getenv('CLAUDE_API_KEY') or os.getenv('ANTHROPIC_API_KEY')
        if not api_key:
            return news_item  # API í‚¤ ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜
        
        api_url = "https://api.anthropic.com/v1/messages"
        headers = {
            "x-api-key": api_key,
            "anthropic-version": "2023-06-01",
            "Content-Type": "application/json"
        }
        
        prompt = f"""ë‹¤ìŒ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ì—¬ ëª¨ë°”ì¼ ê²Œì„ íŠ¸ë˜í”½ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆëŠ”ì§€ íŒë‹¨í•´ì£¼ì„¸ìš”:

ì œëª©: {news_item.get('title', '')}
ë‚´ìš©: {news_item.get('summary', '')}
URL: {news_item.get('url', '')}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
  "relevant": true ë˜ëŠ” false (ëª¨ë°”ì¼ ê²Œì„ íŠ¸ë˜í”½ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìœ¼ë©´ true),
  "category": "ì„¸ë¶€ ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ (ì•„ë˜ ëª©ë¡ ì°¸ê³ )",
  "country": "ê´€ë ¨ êµ­ê°€ëª… (ì—†ìœ¼ë©´ null)",
  "traffic_impact": "íŠ¸ë˜í”½ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ì„¤ëª… (ê°„ë‹¨íˆ)",
  "summary_kr": "í•œêµ­ì–´ë¡œ 2-3ì¤„ ìš”ì•½"
}}

ì„¸ë¶€ ì¹´í…Œê³ ë¦¬ ëª©ë¡ (ì •í™•íˆ í•˜ë‚˜ ì„ íƒ):

ğŸ”´ ì¥ì•  ë° ì°¨ë‹¨ (Outage & Block):
- internet_shutdown: ì¸í„°ë„· ì°¨ë‹¨, í†µì‹  ì¥ì• 
- tech_outage: ì†Œì…œë¯¸ë””ì–´/ì•±ìŠ¤í† ì–´/í´ë¼ìš°ë“œ ì¥ì• 
- power_outage: ì •ì „, ì „ë ¥ ê³µê¸‰ ì¤‘ë‹¨
- censorship: ê²€ì—´, ì•±/ê²Œì„ ê¸ˆì§€
- cyber_attack: ì‚¬ì´ë²„ ê³µê²©, DDoS, í•´í‚¹
- infrastructure_damage: ì¸í”„ë¼ ì†ìƒ, êµëŸ‰/ê±´ë¬¼ ë¶•ê´´

ğŸŸ  ì‚¬íšŒì  ìœ„ê¸° (Social Crisis):
- war_conflict: ì „ìŸ, ë¶„ìŸ, êµ°ì‚¬ ì‘ì „
- terrorism_explosion: í…ŒëŸ¬, í­ë°œ, í­íƒ„ ê³µê²©
- natural_disaster: ì§€ì§„, í™ìˆ˜, íƒœí’, ì‚°ë¶ˆ ë“± ì²œì¬ì§€ë³€
- protest_strike: ì‹œìœ„, íŒŒì—…, í­ë™
- curfew: í†µê¸ˆ, ë´‰ì‡„, ë¹„ìƒì‚¬íƒœ
- pandemic: íŒ¬ë°ë¯¹, ì „ì—¼ë³‘, ê²©ë¦¬
- economic: ê²½ì œ ìœ„ê¸°, ì¸í”Œë ˆì´ì…˜, í†µí™” í‰ê°€ì ˆí•˜

ğŸŸ¢ ì‹œì¦Œ ë° ì¼ì • (Seasonal & Calendar):
- holiday: ê³µíœ´ì¼, ëª…ì ˆ, ì¶•ì œ
- school_calendar: ë°©í•™, ì‹œí—˜ê¸°ê°„ ë“± í•™ì‚¬ì¼ì •
- election: ì„ ê±°, íˆ¬í‘œ, ì •ì¹˜ ì´ë²¤íŠ¸

ğŸ”µ ê²Œì„ ë° ê²½ìŸ (Gaming & Competitor):
- gaming: ê²Œì„ ê´€ë ¨ ë‰´ìŠ¤
- competitor_game: ê²½ìŸ ê²Œì„ ì¶œì‹œ/ì—…ë°ì´íŠ¸
- social_trend: ë°”ì´ëŸ´ íŠ¸ë Œë“œ, ì¸í”Œë£¨ì–¸ì„œ, eìŠ¤í¬ì¸  í† ë„ˆë¨¼íŠ¸
- sports_event: ì›”ë“œì»µ, ì˜¬ë¦¼í”½ ë“± ìŠ¤í¬ì¸  ì´ë²¤íŠ¸
- major_event: ì£¼ìš” ë¬¸í™” í–‰ì‚¬, ê²Œì„ ì»¨ë²¤ì…˜

âšª ê¸°íƒ€:
- other: ë¶„ë¥˜ ë¶ˆê°€

ê´€ë ¨ì´ ì—†ìœ¼ë©´ relevant: falseë¡œ ì„¤ì •í•˜ì„¸ìš”."""

        payload = {
            "model": "claude-3-5-sonnet-20241022",
            "max_tokens": 500,
            "messages": [
                {"role": "user", "content": prompt}
            ]
        }
    
    else:
        return news_item  # ì•Œ ìˆ˜ ì—†ëŠ” API íƒ€ì…ì´ë©´ ì›ë³¸ ë°˜í™˜
    
    try:
        import requests
        response = requests.post(api_url, headers=headers, json=payload, timeout=30)
        
        if response.status_code != 200:
            logger.warning(f"AI API ì˜¤ë¥˜: {response.status_code}, ì›ë³¸ ë‰´ìŠ¤ ì‚¬ìš©")
            return news_item
        
        data = response.json()
        
        # ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        if api_type == 'openai':
            content = data['choices'][0]['message']['content']
        else:  # claude
            content = data['content'][0]['text']
        
        # JSON ì¶”ì¶œ
        import re
        json_match = re.search(r'\{[\s\S]*\}', content)
        if json_match:
            ai_result = json.loads(json_match.group())
            
            # ê´€ë ¨ ì—†ìœ¼ë©´ None ë°˜í™˜
            if not ai_result.get('relevant', False):
                return None
            
            # ì„¸ë¶€ ì¹´í…Œê³ ë¦¬
            detail_category = ai_result.get('category', 'other')
            
            # ì •ì œëœ ì •ë³´ ë³‘í•©
            refined_item = {
                **news_item,
                'category': detail_category,  # ì„¸ë¶€ ì¹´í…Œê³ ë¦¬ ì €ì¥
                'category_group': map_to_group_category(detail_category),  # ê·¸ë£¹ ì¹´í…Œê³ ë¦¬ ì¶”ê°€
                'summary': ai_result.get('summary_kr', news_item.get('summary', '')),
                'traffic_impact': ai_result.get('traffic_impact', '')
            }
            
            # êµ­ê°€ ì •ë³´ ì—…ë°ì´íŠ¸
            if ai_result.get('country'):
                refined_item['country'] = ai_result.get('country')
                refined_item['continent'] = get_continent(ai_result.get('country'))
            
            return refined_item
        else:
            logger.warning("AI ì‘ë‹µì—ì„œ JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì›ë³¸ ì‚¬ìš©")
            return news_item
            
    except Exception as e:
        logger.error(f"AI ì •ì œ ì‹¤íŒ¨: {e}, ì›ë³¸ ë‰´ìŠ¤ ì‚¬ìš©")
        return news_item


def cross_validate_news(openai_news: List[Dict], claude_news: List[Dict]) -> List[Dict]:
    """
    OpenAIì™€ Claude API ê²°ê³¼ë¥¼ êµì°¨ê²€ì¦í•˜ì—¬ ì‹ ë¢°ë„ ë†’ì€ ë‰´ìŠ¤ ë°˜í™˜
    
    Args:
        openai_news: OpenAI APIë¡œ ìˆ˜ì§‘í•œ ë‰´ìŠ¤
        claude_news: Claude APIë¡œ ìˆ˜ì§‘í•œ ë‰´ìŠ¤
    
    Returns:
        êµì°¨ê²€ì¦ëœ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ (ì‹ ë¢°ë„ ì ìˆ˜ í¬í•¨)
    """
    validated_news = []
    seen_titles = set()
    
    # ì œëª© ìœ ì‚¬ë„ ë¹„êµ í•¨ìˆ˜ (ê°„ë‹¨í•œ ë²„ì „)
    def title_similarity(title1: str, title2: str) -> float:
        """ë‘ ì œëª©ì˜ ìœ ì‚¬ë„ ê³„ì‚° (0.0 ~ 1.0)"""
        title1_lower = title1.lower()
        title2_lower = title2.lower()
        
        # ì™„ì „ ì¼ì¹˜
        if title1_lower == title2_lower:
            return 1.0
        
        # ë‹¨ì–´ ê¸°ë°˜ ìœ ì‚¬ë„
        words1 = set(title1_lower.split())
        words2 = set(title2_lower.split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union) if union else 0.0
    
    # OpenAI ë‰´ìŠ¤ ì²˜ë¦¬
    for news in openai_news:
        title = news.get('title', '').lower()
        if title in seen_titles:
            continue
        
        # Claude ê²°ê³¼ì™€ ë¹„êµ
        matched = False
        best_match_score = 0.0
        best_match = None
        
        for claude_item in claude_news:
            claude_title = claude_item.get('title', '').lower()
            similarity = title_similarity(title, claude_title)
            
            if similarity > 0.7:  # 70% ì´ìƒ ìœ ì‚¬í•˜ë©´ ì¼ì¹˜ë¡œ ê°„ì£¼
                matched = True
                if similarity > best_match_score:
                    best_match_score = similarity
                    best_match = claude_item
        
        if matched and best_match:
            # ë‘ APIê°€ ì¼ì¹˜í•˜ëŠ” ë‰´ìŠ¤: ì‹ ë¢°ë„ ë†’ìŒ
            validated_item = news.copy()
            validated_item['confidence'] = 'high'
            validated_item['validation'] = f"OpenAI + Claude ì¼ì¹˜ (ìœ ì‚¬ë„: {best_match_score:.0%})"
            validated_item['openai_summary'] = news.get('summary', '')
            validated_item['claude_summary'] = best_match.get('summary', '')
            # ë” ê¸´ ìš”ì•½ ì‚¬ìš©
            if len(best_match.get('summary', '')) > len(news.get('summary', '')):
                validated_item['summary'] = best_match.get('summary', '')
            validated_news.append(validated_item)
            seen_titles.add(title)
            seen_titles.add(best_match.get('title', '').lower())
        else:
            # OpenAIë§Œ ì°¾ì€ ë‰´ìŠ¤: ì‹ ë¢°ë„ ì¤‘ê°„
            validated_item = news.copy()
            validated_item['confidence'] = 'medium'
            validated_item['validation'] = 'OpenAI only'
            validated_news.append(validated_item)
            seen_titles.add(title)
    
    # Claudeë§Œ ì°¾ì€ ë‰´ìŠ¤ ì¶”ê°€
    for news in claude_news:
        title = news.get('title', '').lower()
        if title not in seen_titles:
            validated_item = news.copy()
            validated_item['confidence'] = 'medium'
            validated_item['validation'] = 'Claude only'
            validated_news.append(validated_item)
            seen_titles.add(title)
    
    # ì‹ ë¢°ë„ ìˆœìœ¼ë¡œ ì •ë ¬ (high > medium)
    validated_news.sort(key=lambda x: (x.get('confidence') == 'high', x.get('title', '')))
    
    logger.info(f"êµì°¨ê²€ì¦ ì™„ë£Œ: ì´ {len(validated_news)}ê°œ (High: {sum(1 for n in validated_news if n.get('confidence') == 'high')}, Medium: {sum(1 for n in validated_news if n.get('confidence') == 'medium')})")
    
    return validated_news


def fetch_news_with_cross_validation(keyword: str, countries: List[Dict] = None) -> List[Dict]:
    """
    OpenAIì™€ Claude APIë¥¼ ëª¨ë‘ ì‚¬ìš©í•˜ì—¬ êµì°¨ê²€ì¦
    
    Args:
        keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
        countries: ê´€ë ¨ êµ­ê°€ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        êµì°¨ê²€ì¦ëœ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    logger.info(f"êµì°¨ê²€ì¦ ì‹œì‘: {keyword}")
    
    # ë‘ API ëª¨ë‘ í˜¸ì¶œ
    openai_news = fetch_news_from_openai(keyword, countries)
    time.sleep(1)  # API ë¶€í•˜ ë°©ì§€
    claude_news = fetch_news_from_claude(keyword, countries)
    
    # êµì°¨ê²€ì¦
    if openai_news or claude_news:
        validated = cross_validate_news(openai_news, claude_news)
        return validated
    else:
        # ë‘˜ ë‹¤ ì‹¤íŒ¨ ì‹œ RSS í´ë°±
        logger.warning("OpenAIì™€ Claude ëª¨ë‘ ì‹¤íŒ¨, RSSë¡œ í´ë°±")
        return fetch_news_from_rss(keyword)


def remove_duplicates(existing_news: List[Dict], new_news: List[Dict]) -> List[Dict]:
    """
    ì¤‘ë³µ ë‰´ìŠ¤ ì œê±° (URL ê¸°ì¤€)
    
    Args:
        existing_news: ê¸°ì¡´ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        new_news: ìƒˆë¡œ ìˆ˜ì§‘í•œ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        ì¤‘ë³µ ì œê±°ëœ ìƒˆ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    existing_urls = {news['url'] for news in existing_news if news.get('url')}
    unique_news = [news for news in new_news if news.get('url') not in existing_urls]
    
    logger.info(f"ì¤‘ë³µ ì œê±°: {len(new_news)}ê°œ ì¤‘ {len(unique_news)}ê°œ ìœ ë‹ˆí¬")
    return unique_news


def load_existing_news() -> List[Dict]:
    """ê¸°ì¡´ CSV íŒŒì¼ì—ì„œ ë‰´ìŠ¤ ë¡œë“œ"""
    if not NEWS_CSV.exists():
        logger.info("ê¸°ì¡´ ë‰´ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
        return []
    
    try:
        df = pd.read_csv(NEWS_CSV, encoding='utf-8-sig')
        return df.to_dict('records')
    except Exception as e:
        logger.error(f"ê¸°ì¡´ ë‰´ìŠ¤ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
        return []


def save_to_csv(all_news: List[Dict]):
    """
    ë‰´ìŠ¤ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥
    
    Args:
        all_news: ì €ì¥í•  ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    if not all_news:
        logger.warning("ì €ì¥í•  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    try:
        # ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±
        DATA_DIR.mkdir(exist_ok=True)
        
        # DataFrame ìƒì„±
        df = pd.DataFrame(all_news)
        
        # ì»¬ëŸ¼ ìˆœì„œ ì§€ì • (êµì°¨ê²€ì¦ ì»¬ëŸ¼ í¬í•¨)
        base_columns = ['date', 'country', 'continent', 'title', 'summary', 'url', 'source', 'category', 'category_group', 'traffic_impact']
        optional_columns = ['confidence', 'validation', 'openai_summary', 'claude_summary']
        
        # ëª¨ë“  ì»¬ëŸ¼ í™•ì¸
        all_columns = base_columns + [col for col in optional_columns if col in df.columns]
        df = df.reindex(columns=all_columns)
        
        # ë‚ ì§œìˆœ ì •ë ¬ (ìµœì‹ ìˆœ)
        df['date'] = pd.to_datetime(df['date'])
        df = df.sort_values('date', ascending=False)
        df['date'] = df['date'].dt.strftime('%Y-%m-%d')
        
        # CSV ì €ì¥ (UTF-8 with BOM for Excel compatibility)
        df.to_csv(NEWS_CSV, index=False, encoding='utf-8-sig', quoting=csv.QUOTE_MINIMAL)
        
        logger.info(f"ë‰´ìŠ¤ {len(all_news)}ê°œë¥¼ {NEWS_CSV}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        logger.error(f"CSV ì €ì¥ ì‹¤íŒ¨: {e}")
        raise


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger.info("=" * 50)
    logger.info("ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘ (RSS â†’ AI ì •ì œ ë°©ì‹)")
    logger.info("=" * 50)
    
    try:
        # í‚¤ì›Œë“œ ë¡œë“œ
        keywords_config = load_keywords()
        base_keywords = keywords_config.get('base_keywords', [])
        priority_countries = keywords_config.get('priority_countries', {})
        traffic_impact_keywords = keywords_config.get('traffic_impact_keywords', {})
        
        # ê¸°ì¡´ ë‰´ìŠ¤ ë¡œë“œ
        existing_news = load_existing_news()
        logger.info(f"ê¸°ì¡´ ë‰´ìŠ¤: {len(existing_news)}ê°œ")
        
        # 1ë‹¨ê³„: RSSë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘
        logger.info("=" * 50)
        logger.info("1ë‹¨ê³„: RSSë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
        logger.info("=" * 50)
        
        all_raw_news = []
        
        # ê¸°ë³¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
        for keyword in base_keywords:
            news = fetch_news_from_rss(keyword)
            all_raw_news.extend(news)
            time.sleep(1)  # API ë¶€í•˜ ë°©ì§€
        
        # ì£¼ìš” êµ­ê°€ë³„ ê²€ìƒ‰
        for country, country_info in priority_countries.items():
            # êµ­ê°€ í‚¤ì›Œë“œ
            for keyword in country_info.get('keywords', []):
                news = fetch_news_from_rss(keyword)
                for item in news:
                    item['country'] = country
                    item['continent'] = get_continent(country)
                all_raw_news.extend(news)
                time.sleep(1)
            
            # êµ­ê°€ë³„ ì£¼ì œ í‚¤ì›Œë“œ
            for topic in country_info.get('topics', []):
                keyword = f"{country} {topic}"
                news = fetch_news_from_rss(keyword)
                for item in news:
                    item['country'] = country
                    item['continent'] = get_continent(country)
                all_raw_news.extend(news)
                time.sleep(1)
        
        # íŠ¸ë˜í”½ ì˜í–¥ í‚¤ì›Œë“œ ê²€ìƒ‰
        for category, keywords in traffic_impact_keywords.items():
            for keyword in keywords[:3]:  # ê° ì¹´í…Œê³ ë¦¬ë‹¹ ìµœëŒ€ 3ê°œ í‚¤ì›Œë“œ
                news = fetch_news_from_rss(keyword)
                all_raw_news.extend(news)
                time.sleep(1)
        
        logger.info(f"RSS ìˆ˜ì§‘ ì™„ë£Œ: {len(all_raw_news)}ê°œ ë‰´ìŠ¤")
        
        # 2ë‹¨ê³„: AIë¡œ ì •ì œ
        logger.info("=" * 50)
        logger.info("2ë‹¨ê³„: AIë¡œ ë‰´ìŠ¤ ì •ì œ ì¤‘...")
        logger.info("=" * 50)
        
        # API íƒ€ì… ê²°ì •
        api_type = os.getenv('NEWS_API_TYPE', 'openai').lower()
        if api_type not in ['openai', 'claude']:
            api_type = 'openai'
        
        logger.info(f"ì‚¬ìš©í•  AI API: {api_type.upper()}")
        
        all_refined_news = []
        skipped_count = 0
        
        for i, news_item in enumerate(all_raw_news, 1):
            if i % 10 == 0:
                logger.info(f"ì •ì œ ì§„í–‰ ì¤‘: {i}/{len(all_raw_news)}")
            
            refined = refine_news_with_ai(news_item, api_type)
            
            if refined is None:
                skipped_count += 1
            else:
                all_refined_news.append(refined)
            
            time.sleep(1)  # API Rate Limit ë°©ì§€
        
        logger.info(f"AI ì •ì œ ì™„ë£Œ: {len(all_refined_news)}ê°œ (ìŠ¤í‚µ: {skipped_count}ê°œ)")
        
        # ì¤‘ë³µ ì œê±°
        unique_new_news = remove_duplicates(existing_news, all_refined_news)
        
        # ê¸°ì¡´ ë‰´ìŠ¤ì™€ í•©ì¹˜ê¸°
        all_news = existing_news + unique_new_news
        
        # ì €ì¥
        if unique_new_news:
            save_to_csv(all_news)
            logger.info(f"âœ… {len(unique_new_news)}ê°œì˜ ìƒˆ ë‰´ìŠ¤ë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
            return 0  # ì„±ê³µ (ë³€ê²½ì‚¬í•­ ìˆìŒ)
        else:
            logger.info("ìƒˆë¡œìš´ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return 1  # ë³€ê²½ì‚¬í•­ ì—†ìŒ
            
    except Exception as e:
        logger.error(f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        return -1  # ì‹¤íŒ¨


if __name__ == '__main__':
    exit_code = main()
    sys.exit(exit_code)

